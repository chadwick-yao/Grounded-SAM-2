{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from supervision.draw.color import ColorPalette\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.9\"\n",
    "\n",
    "def load_models(dino_id=\"IDEA-Research/grounding-dino-base\", sam2_id=\"facebook/sam2-hiera-large\"):\n",
    "    mask_predictor = SAM2ImagePredictor.from_pretrained(sam2_id, device=device)\n",
    "    grounding_processor = AutoProcessor.from_pretrained(dino_id)\n",
    "    grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_id).to(device)\n",
    "    \n",
    "    return mask_predictor, grounding_processor, grounding_model\n",
    "\n",
    "mask_predictor, grounding_processor, grounding_model = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table. chair. trash can.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_list = [\"table\", \"chair\", \"trash can\"]\n",
    "text_prompt = \". \".join(object_list) + \".\"\n",
    "text_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  20%|██        | 609/2972 [04:55<1:14:56,  1.90s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "img_dir = \"/home/chadwick/Downloads/image\"\n",
    "img_dir = pathlib.Path(img_dir)\n",
    "img_paths = list(img_dir.glob(\"*.png\"))\n",
    "\n",
    "for img_path in tqdm(img_paths, desc=\"Processing images\"):\n",
    "    image = Image.open(img_path)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    inputs = grounding_processor(\n",
    "        images=image,\n",
    "        text=text_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = grounding_model(**inputs)\n",
    "\n",
    "    results = grounding_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.4,\n",
    "        text_threshold=0.4,\n",
    "        target_sizes=[image.shape[:2]],\n",
    "    )\n",
    "\n",
    "    class_names: list = results[0][\"labels\"]\n",
    "    input_boxes = results[0][\"boxes\"].cpu().numpy()\n",
    "    confidences = results[0][\"scores\"].cpu().numpy().tolist()\n",
    "\n",
    "    mask_predictor.set_image(image)\n",
    "\n",
    "    masks, _, _ = mask_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence in zip(class_names, confidences)\n",
    "    ]\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids,\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator(color=ColorPalette.DEFAULT)\n",
    "    annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator(color=ColorPalette.DEFAULT)\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame, detections=detections, labels=labels\n",
    "    )\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator(color=ColorPalette.DEFAULT)\n",
    "    annotated_frame = mask_annotator.annotate(\n",
    "        scene=annotated_frame, detections=detections\n",
    "    )\n",
    "\n",
    "    # save annotated frame, numpy array h x w x 3\n",
    "    try:\n",
    "        Image.fromarray(annotated_frame).save(\n",
    "            pathlib.Path(\"/home/chadwick/Downloads/image_s\") / f\"{img_path.stem}.png\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")\n",
    "\n",
    "    # save class_names, input_boxes, masks, and confidences into npz file\n",
    "    np.savez(\n",
    "        pathlib.Path(\"/home/chadwick/Downloads/image_npz\") / f\"{img_path.stem}.npz\",\n",
    "        labels=class_names,\n",
    "        bboxes=input_boxes,\n",
    "        masks=masks,\n",
    "        confidences=confidences,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
