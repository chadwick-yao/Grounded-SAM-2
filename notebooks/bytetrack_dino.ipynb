{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.9\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import supervision as sv\n",
    "from supervision.draw.color import ColorPalette\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "dino_id=\"IDEA-Research/grounding-dino-base\"\n",
    "grounding_processor = AutoProcessor.from_pretrained(dino_id)\n",
    "grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table. chair. trash can.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_list = [\"table\", \"chair\", \"trash can\"]\n",
    "text_prompt = \". \".join(object_list) + \".\"\n",
    "text_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import rospy\n",
    "import rosbag\n",
    "from cv_bridge import CvBridge\n",
    "import pathlib\n",
    "import cv2\n",
    "\n",
    "from byte_tracker import BYTETracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "# args for BYTETracker\n",
    "args = SimpleNamespace(**{\n",
    "        \"track_thresh\": 0.5,\n",
    "        \"track_buffer\": 30,\n",
    "        \"match_thresh\": 0.75,\n",
    "        \"mot20\": False,\n",
    "        \"min_box_area\": 100,\n",
    "})\n",
    "\n",
    "tracker = BYTETracker(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing messages:   1%|          | 17/2972 [00:02<06:44,  7.31it/s]\n"
     ]
    }
   ],
   "source": [
    "rospy.init_node(\"image_extractor\", anonymous=True)\n",
    "bridge = CvBridge()\n",
    "\n",
    "bag_file = \"/home/chadwick/Downloads/system.bag\"\n",
    "result_filename = \"./results.txt\"\n",
    "\n",
    "\n",
    "with rosbag.Bag(bag_file, \"r\") as bag:\n",
    "    total_messages = bag.get_message_count(\"/camera/image\")\n",
    "    for topic, msg, t in tqdm(\n",
    "        bag.read_messages(topics=[\"/camera/image\"]),\n",
    "        total=total_messages,\n",
    "        desc=\"Processing messages\",\n",
    "    ):\n",
    "        cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding=\"bgr8\")\n",
    "        rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = rgb_image.shape\n",
    "        img_size = (height, width)\n",
    "        image = np.array(rgb_image)\n",
    "\n",
    "        inputs = grounding_processor(\n",
    "            images=image,\n",
    "            text=text_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = grounding_model(**inputs)\n",
    "\n",
    "        results = grounding_processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=0.4,\n",
    "            text_threshold=0.4,\n",
    "            target_sizes=[image.shape[:2]],\n",
    "        )\n",
    "\n",
    "        class_names: list = results[0][\"labels\"]\n",
    "        input_boxes = results[0][\"boxes\"].cpu().numpy()  # (n_boxes, 4)\n",
    "        confidences = results[0][\"scores\"].cpu().numpy()  # (n_boxes,)\n",
    "        detection_data = np.hstack((input_boxes, confidences.reshape(-1, 1)))\n",
    "\n",
    "        class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "        labels = [\n",
    "            f\"{class_name} {confidence:.2f}\"\n",
    "            for class_name, confidence in zip(class_names, confidences)\n",
    "        ]\n",
    "\n",
    "        detections = sv.Detections(\n",
    "            xyxy=input_boxes,  # (n, 4)\n",
    "            class_id=class_ids,\n",
    "        )\n",
    "\n",
    "        box_annotator = sv.BoxAnnotator(color=ColorPalette.DEFAULT)\n",
    "        annotated_frame = box_annotator.annotate(\n",
    "            scene=image.copy(), detections=detections\n",
    "        )\n",
    "\n",
    "        label_annotator = sv.LabelAnnotator(color=ColorPalette.DEFAULT)\n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame, detections=detections, labels=labels\n",
    "        )\n",
    "\n",
    "        online_targets = tracker.update(detection_data, img_size, img_size)\n",
    "        online_tlwhs = []\n",
    "        online_tlbrs = []\n",
    "        online_ids = []\n",
    "        online_scores = []\n",
    "\n",
    "        for t in online_targets:\n",
    "            tlwh = t.tlwh\n",
    "            tlbr = t.tlbr\n",
    "            tid = t.track_id\n",
    "            vertical = tlwh[2] / tlwh[3] > 1.6\n",
    "            if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n",
    "                online_tlwhs.append(tlwh)\n",
    "                online_tlbrs.append(tlbr)\n",
    "                online_ids.append(tid)\n",
    "                online_scores.append(t.score)\n",
    "\n",
    "        detections = sv.Detections(\n",
    "            xyxy=np.array(online_tlbrs),\n",
    "            class_id=np.array(online_ids),\n",
    "        )\n",
    "\n",
    "        box_annotator = sv.BoxAnnotator(color=ColorPalette.DEFAULT)\n",
    "        annotated_frame_1 = box_annotator.annotate(\n",
    "            scene=image.copy(), detections=detections\n",
    "        )\n",
    "\n",
    "        labels = [\n",
    "            f\"{class_name} {confidence:.2f}\"\n",
    "            for class_name, confidence in zip(online_ids, online_scores)\n",
    "        ]\n",
    "\n",
    "        label_annotator = sv.LabelAnnotator(color=ColorPalette.DEFAULT)\n",
    "        annotated_frame_1 = label_annotator.annotate(\n",
    "            scene=annotated_frame_1, detections=detections, labels=labels\n",
    "        )\n",
    "\n",
    "        # show the annotated frame using cv2\n",
    "        final_frame = np.vstack((annotated_frame, annotated_frame_1))\n",
    "        final_frame = cv2.cvtColor(final_frame, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow(\"Annotated Frame\", final_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
